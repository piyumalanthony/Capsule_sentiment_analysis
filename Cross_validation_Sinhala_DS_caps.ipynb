{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross_validation_Sinhala_DS_caps.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "environment": {
      "name": "tf-gpu.1-15.m50",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m50"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgRvEbMXy9nc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1ysHXd6zBB0"
      },
      "source": [
        "cd /content/drive/My Drive/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/Capsule/Dynamic _rounting_enhancement_Capsule_network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RJTK5LXWz6Z"
      },
      "source": [
        "!pip install tensorflow==1.14.0 \n",
        "!pip install keras==2.1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaXzdTOiXKMR"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqkei8O4Tpgd"
      },
      "source": [
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import h5py\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from tensorflow.contrib.layers.python.layers import initializers\n",
        "\n",
        "import collections\n",
        "import pickle\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import sys\n",
        "import os \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Dropout, Activation, Flatten,Embedding, Convolution1D, MaxPooling1D, AveragePooling1D, Input, Dense, merge,Add\n",
        "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
        "from keras.regularizers import l2\n",
        "from keras.constraints import maxnorm\n",
        "from keras.datasets import imdb\n",
        "from keras import callbacks\n",
        "from keras.utils import generic_utils\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adadelta\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import sys, re\n",
        "import pandas as pd\n",
        "\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CztzxeaRXMat"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ3p07C8acEI"
      },
      "source": [
        "class Classifer(keras.layers.Layer):\n",
        "\n",
        "    def get_hidden_states_before(self, hidden_states, step, shape, hidden_size):\n",
        "        #padding zeros\n",
        "        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n",
        "        #remove last steps\n",
        "        displaced_hidden_states=hidden_states[:,:-step,:]\n",
        "        #concat padding\n",
        "        return tf.concat([padding, displaced_hidden_states], axis=1)\n",
        "        #return tf.cond(step<=shape[1], lambda: tf.concat([padding, displaced_hidden_states], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n",
        "\n",
        "    def get_hidden_states_after(self, hidden_states, step, shape, hidden_size):\n",
        "        #padding zeros\n",
        "        padding=tf.zeros((shape[0], step, hidden_size), dtype=tf.float32)\n",
        "        #remove last steps\n",
        "        displaced_hidden_states=hidden_states[:,step:,:]\n",
        "        #concat padding\n",
        "        return tf.concat([displaced_hidden_states, padding], axis=1)\n",
        "        #return tf.cond(step<=shape[1], lambda: tf.concat([displaced_hidden_states, padding], axis=1), lambda: tf.zeros((shape[0], shape[1], self.config.hidden_size_sum), dtype=tf.float32))\n",
        "\n",
        "    def sum_together(self, l):\n",
        "        combined_state=None\n",
        "        for tensor in l:\n",
        "            if combined_state==None:\n",
        "                combined_state=tensor\n",
        "            else:\n",
        "                combined_state=combined_state+tensor\n",
        "        return combined_state\n",
        "    \n",
        "    def slstm_cell(self, name_scope_name, hidden_size, lengths, initial_hidden_states, initial_cell_states, num_layers):\n",
        "        with tf.name_scope(name_scope_name):\n",
        "            #Word parameters \n",
        "            #forget gate for left \n",
        "            with tf.name_scope(\"f1_gate\"):\n",
        "                #current\n",
        "                Wxf1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n",
        "                #left right\n",
        "                Whf1 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n",
        "                #initial state\n",
        "                Wif1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n",
        "                #dummy node\n",
        "                Wdf1 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n",
        "            #forget gate for right \n",
        "            with tf.name_scope(\"f2_gate\"):\n",
        "                Wxf2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n",
        "                Whf2 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n",
        "                Wif2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n",
        "                Wdf2 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n",
        "            #forget gate for inital states     \n",
        "            with tf.name_scope(\"f3_gate\"):\n",
        "                Wxf3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n",
        "                Whf3 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n",
        "                Wif3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n",
        "                Wdf3 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n",
        "            #forget gate for dummy states     \n",
        "            with tf.name_scope(\"f4_gate\"):\n",
        "                Wxf4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n",
        "                Whf4 = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n",
        "                Wif4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wif\")\n",
        "                Wdf4 = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdf\")\n",
        "            #input gate for current state     \n",
        "            with tf.name_scope(\"i_gate\"):\n",
        "                Wxi = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxi\")\n",
        "                Whi = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whi\")\n",
        "                Wii = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wii\")\n",
        "                Wdi = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdi\")\n",
        "            #input gate for output gate\n",
        "            with tf.name_scope(\"o_gate\"):\n",
        "                Wxo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n",
        "                Who = tf.Variable(tf.random_normal([2*hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n",
        "                Wio = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wio\")\n",
        "                Wdo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wdo\")\n",
        "            #bias for the gates    \n",
        "            with tf.name_scope(\"biases\"):\n",
        "                bi = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n",
        "                bo = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n",
        "                bf1 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf1\")\n",
        "                bf2 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf2\")\n",
        "                bf3 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf3\")\n",
        "                bf4 = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bf4\")\n",
        "\n",
        "            #dummy node gated attention parameters\n",
        "            #input gate for dummy state\n",
        "            with tf.name_scope(\"gated_d_gate\"):\n",
        "                gated_Wxd = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxf\")\n",
        "                gated_Whd = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Whf\")\n",
        "            #output gate\n",
        "            with tf.name_scope(\"gated_o_gate\"):\n",
        "                gated_Wxo = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n",
        "                gated_Who = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n",
        "            #forget gate for states of word\n",
        "            with tf.name_scope(\"gated_f_gate\"):\n",
        "                gated_Wxf = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Wxo\")\n",
        "                gated_Whf = tf.Variable(tf.random_normal([hidden_size, hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"Who\")\n",
        "            #biases\n",
        "            with tf.name_scope(\"gated_biases\"):\n",
        "                gated_bd = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bi\")\n",
        "                gated_bo = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n",
        "                gated_bf = tf.Variable(tf.random_normal([hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"bo\")\n",
        "        # print(\"first phase done!\")\n",
        "        #filters for attention        \n",
        "        mask_softmax_score=tf.cast(tf.sequence_mask(lengths), tf.float32)*1e25-1e25\n",
        "        # print(\"second phase done!\")\n",
        "        # print(mask_softmax_score.shape)\n",
        "        mask_softmax_score_expanded=tf.expand_dims(mask_softmax_score, axis=2)\n",
        "        # print(\"third phase done!\")               \n",
        "        #filter invalid steps\n",
        "        sequence_mask=tf.expand_dims(tf.cast(tf.sequence_mask(lengths), tf.float32),axis=2)\n",
        "        # print(\"fourth phase done!\")\n",
        "        # print(initial_hidden_states.shape)\n",
        "        # print(sequence_mask.shape)\n",
        "        #filter embedding states\n",
        "        initial_hidden_states=initial_hidden_states*sequence_mask\n",
        "        # print(\"fifth phase done!\")\n",
        "        initial_cell_states=initial_cell_states*sequence_mask\n",
        "        # print(\"sixth phase done!\")\n",
        "        #record shape of the batch\n",
        "        shape=tf.shape(initial_hidden_states)\n",
        "        \n",
        "        #initial embedding states\n",
        "        embedding_hidden_state=tf.reshape(initial_hidden_states, [-1, hidden_size])      \n",
        "        embedding_cell_state=tf.reshape(initial_cell_states, [-1, hidden_size])\n",
        "\n",
        "        #randomly initialize the states\n",
        "        if config.random_initialize:\n",
        "            initial_hidden_states=tf.random_uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n",
        "            initial_cell_states=tf.random_uniform(shape, minval=-0.05, maxval=0.05, dtype=tf.float32, seed=None, name=None)\n",
        "            #filter it\n",
        "            initial_hidden_states=initial_hidden_states*sequence_mask\n",
        "            initial_cell_states=initial_cell_states*sequence_mask\n",
        "\n",
        "        #inital dummy node states\n",
        "        dummynode_hidden_states=tf.reduce_mean(initial_hidden_states, axis=1)\n",
        "        dummynode_cell_states=tf.reduce_mean(initial_cell_states, axis=1)\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            #update dummy node states\n",
        "            #average states\n",
        "            combined_word_hidden_state=tf.reduce_mean(initial_hidden_states, axis=1)\n",
        "            reshaped_hidden_output=tf.reshape(initial_hidden_states, [-1, hidden_size])\n",
        "            #copy dummy states for computing forget gate\n",
        "            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n",
        "            #input gate\n",
        "            gated_d_t = tf.nn.sigmoid(\n",
        "                tf.matmul(dummynode_hidden_states, gated_Wxd) + tf.matmul(combined_word_hidden_state, gated_Whd) + gated_bd\n",
        "            )\n",
        "            #output gate\n",
        "            gated_o_t = tf.nn.sigmoid(\n",
        "                tf.matmul(dummynode_hidden_states, gated_Wxo) + tf.matmul(combined_word_hidden_state, gated_Who) + gated_bo\n",
        "            )\n",
        "            #forget gate for hidden states\n",
        "            gated_f_t = tf.nn.sigmoid(\n",
        "                tf.matmul(transformed_dummynode_hidden_states, gated_Wxf) + tf.matmul(reshaped_hidden_output, gated_Whf) + gated_bf\n",
        "            )\n",
        "\n",
        "            #softmax on each hidden dimension \n",
        "            reshaped_gated_f_t=tf.reshape(gated_f_t, [shape[0], shape[1], hidden_size])+ mask_softmax_score_expanded\n",
        "            gated_softmax_scores=tf.nn.softmax(tf.concat([reshaped_gated_f_t, tf.expand_dims(gated_d_t, dim=1)], axis=1), dim=1)\n",
        "            #split the softmax scores\n",
        "            new_reshaped_gated_f_t=gated_softmax_scores[:,:shape[1],:]\n",
        "            new_gated_d_t=gated_softmax_scores[:,shape[1]:,:]\n",
        "            #new dummy states\n",
        "            dummy_c_t=tf.reduce_sum(new_reshaped_gated_f_t * initial_cell_states, axis=1) + tf.squeeze(new_gated_d_t, axis=1)*dummynode_cell_states\n",
        "            dummy_h_t=gated_o_t * tf.nn.tanh(dummy_c_t)\n",
        "\n",
        "            #update word node states\n",
        "            #get states before\n",
        "            initial_hidden_states_before=[tf.reshape(self.get_hidden_states_before(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n",
        "            initial_hidden_states_before=self.sum_together(initial_hidden_states_before)\n",
        "            initial_hidden_states_after= [tf.reshape(self.get_hidden_states_after(initial_hidden_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n",
        "            initial_hidden_states_after=self.sum_together(initial_hidden_states_after)\n",
        "            #get states after\n",
        "            initial_cell_states_before=[tf.reshape(self.get_hidden_states_before(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n",
        "            initial_cell_states_before=self.sum_together(initial_cell_states_before)\n",
        "            initial_cell_states_after=[tf.reshape(self.get_hidden_states_after(initial_cell_states, step+1, shape, hidden_size), [-1, hidden_size]) for step in range(self.config.step)]\n",
        "            initial_cell_states_after=self.sum_together(initial_cell_states_after)\n",
        "            \n",
        "            #reshape for matmul\n",
        "            initial_hidden_states=tf.reshape(initial_hidden_states, [-1, hidden_size])\n",
        "            initial_cell_states=tf.reshape(initial_cell_states, [-1, hidden_size])\n",
        "\n",
        "            #concat before and after hidden states\n",
        "            concat_before_after=tf.concat([initial_hidden_states_before, initial_hidden_states_after], axis=1)\n",
        "\n",
        "            #copy dummy node states \n",
        "            transformed_dummynode_hidden_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_hidden_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n",
        "            transformed_dummynode_cell_states=tf.reshape(tf.tile(tf.expand_dims(dummynode_cell_states, axis=1), [1, shape[1],1]), [-1, hidden_size])\n",
        "\n",
        "            f1_t = tf.nn.sigmoid(\n",
        "                tf.matmul(initial_hidden_states, Wxf1) + tf.matmul(concat_before_after, Whf1) + \n",
        "                tf.matmul(embedding_hidden_state, Wif1) + tf.matmul(transformed_dummynode_hidden_states, Wdf1)+ bf1\n",
        "            )\n",
        "\n",
        "            f2_t = tf.nn.sigmoid(\n",
        "                tf.matmul(initial_hidden_states, Wxf2) + tf.matmul(concat_before_after, Whf2) + \n",
        "                tf.matmul(embedding_hidden_state, Wif2) + tf.matmul(transformed_dummynode_hidden_states, Wdf2)+ bf2\n",
        "            )\n",
        "\n",
        "            f3_t = tf.nn.sigmoid(\n",
        "                tf.matmul(initial_hidden_states, Wxf3) + tf.matmul(concat_before_after, Whf3) + \n",
        "                tf.matmul(embedding_hidden_state, Wif3) + tf.matmul(transformed_dummynode_hidden_states, Wdf3) + bf3\n",
        "            )\n",
        "\n",
        "            f4_t = tf.nn.sigmoid(\n",
        "                tf.matmul(initial_hidden_states, Wxf4) + tf.matmul(concat_before_after, Whf4) + \n",
        "                tf.matmul(embedding_hidden_state, Wif4) + tf.matmul(transformed_dummynode_hidden_states, Wdf4) + bf4\n",
        "            )\n",
        "            \n",
        "            i_t = tf.nn.sigmoid(\n",
        "                tf.matmul(initial_hidden_states, Wxi) + tf.matmul(concat_before_after, Whi) + \n",
        "                tf.matmul(embedding_hidden_state, Wii) + tf.matmul(transformed_dummynode_hidden_states, Wdi)+ bi\n",
        "            )\n",
        "            \n",
        "            o_t = tf.nn.sigmoid(\n",
        "                tf.matmul(initial_hidden_states, Wxo) + tf.matmul(concat_before_after, Who) + \n",
        "                tf.matmul(embedding_hidden_state, Wio) + tf.matmul(transformed_dummynode_hidden_states, Wdo) + bo\n",
        "            )\n",
        "            \n",
        "            f1_t, f2_t, f3_t, f4_t, i_t=tf.expand_dims(f1_t, axis=1), tf.expand_dims(f2_t, axis=1),tf.expand_dims(f3_t, axis=1), tf.expand_dims(f4_t, axis=1), tf.expand_dims(i_t, axis=1)\n",
        "\n",
        "\n",
        "            five_gates=tf.concat([f1_t, f2_t, f3_t, f4_t,i_t], axis=1)\n",
        "            five_gates=tf.nn.softmax(five_gates, dim=1)\n",
        "            f1_t,f2_t,f3_t, f4_t,i_t= tf.split(five_gates, num_or_size_splits=5, axis=1)\n",
        "            \n",
        "            f1_t, f2_t, f3_t, f4_t, i_t=tf.squeeze(f1_t, axis=1), tf.squeeze(f2_t, axis=1),tf.squeeze(f3_t, axis=1), tf.squeeze(f4_t, axis=1),tf.squeeze(i_t, axis=1)\n",
        "\n",
        "            c_t = (f1_t * initial_cell_states_before) + (f2_t * initial_cell_states_after)+(f3_t * embedding_cell_state)+ (f4_t * transformed_dummynode_cell_states)+ (i_t * initial_cell_states)\n",
        "            \n",
        "            h_t = o_t * tf.nn.tanh(c_t)\n",
        "\n",
        "            #update states\n",
        "            initial_hidden_states=tf.reshape(h_t, [shape[0], shape[1], hidden_size])\n",
        "            initial_cell_states=tf.reshape(c_t, [shape[0], shape[1], hidden_size])\n",
        "            initial_hidden_states=initial_hidden_states*sequence_mask\n",
        "            initial_cell_states=initial_cell_states*sequence_mask\n",
        "\n",
        "            dummynode_hidden_states=dummy_h_t\n",
        "            dummynode_cell_states=dummy_c_t\n",
        "\n",
        "        initial_hidden_states = tf.nn.dropout(initial_hidden_states,self.dropout)\n",
        "        initial_cell_states = tf.nn.dropout(initial_cell_states, self.dropout)\n",
        "\n",
        "        return initial_hidden_states, initial_cell_states, dummynode_hidden_states\n",
        "\n",
        "\n",
        "    def slstm_basic_layer(self,embedding, config, mask):\n",
        "        embedding = tf.squeeze(embedding)\n",
        "        initial_hidden_states = embedding\n",
        "        # print(\"embedding shape\")\n",
        "        # print(embedding.shape)\n",
        "        initial_cell_states=tf.identity(initial_hidden_states)\n",
        "\n",
        "        initial_hidden_states = tf.nn.dropout(initial_hidden_states,config.keep_prob)\n",
        "        # print(\"initial shape\")\n",
        "        # print(initial_hidden_states.shape)\n",
        "        initial_cell_states = tf.nn.dropout(initial_cell_states, config.keep_prob)\n",
        "\n",
        "        #create layers \n",
        "        \n",
        "        new_hidden_states,new_cell_state, dummynode_hidden_states = self.slstm_cell(\"word_slstm\", config.hidden_size, mask, initial_hidden_states, initial_cell_states, config.layer)\n",
        "        \n",
        "        softmax_w = tf.Variable(tf.random_normal([2*config.hidden_size, config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w\")\n",
        "        softmax_b = tf.Variable(tf.random_normal([config.num_label], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b\")\n",
        "        #representation=dummynode_hidden_states\n",
        "        representation=tf.reduce_mean(tf.concat([new_hidden_states, tf.expand_dims(dummynode_hidden_states, axis=1)], axis=1), axis=1)\n",
        "        representation1=tf.reduce_mean(tf.concat([new_hidden_states, tf.expand_dims(dummynode_hidden_states, axis=1)], axis=1), axis=1)\n",
        "        # new_hidden_states_concat = tf.concat([new_hidden_states, tf.expand_dims(dummynode_hidden_states, axis=1)], axis=1)\n",
        "        softmax_w2 = tf.Variable(tf.random_normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n",
        "        softmax_b2 = tf.Variable(tf.random_normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n",
        "        softmax_w22 = tf.Variable(tf.random_normal([config.hidden_size, 2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_w2\")\n",
        "        softmax_b22 = tf.Variable(tf.random_normal([2*config.hidden_size], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32, name=\"softmax_b2\")\n",
        "        representation=tf.nn.tanh(tf.matmul(representation, softmax_w2)+softmax_b2)\n",
        "        representation1=tf.nn.tanh(tf.matmul(representation1, softmax_w22)+softmax_b22)\n",
        "        \n",
        "        return new_hidden_states, representation, representation1 \n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config=config\n",
        "        self.dropout=self.keep_prob=config.keep_prob\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbNCNX4ZZ35C"
      },
      "source": [
        "def _matmul_broadcast(x, y, name):\n",
        "  \"\"\"Compute x @ y, broadcasting over the first `N - 2` ranks.\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(name) as scope:\n",
        "    return tf.reduce_sum(\n",
        "      tf.nn.dropout(x[..., tf.newaxis] * y[..., tf.newaxis, :, :],1), axis=-2\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99CU41NOax_m"
      },
      "source": [
        "z = tf.constant(0.1, shape=[2,3,4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3fTGFila56V"
      },
      "source": [
        "z[...,tf.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z06E9a1aBVb"
      },
      "source": [
        "def _conv2d_wrapper(inputs, shape, strides, padding, add_bias, activation_fn, name, stddev=0.1):\n",
        "  \"\"\"Wrapper over tf.nn.conv2d().\n",
        "  \"\"\"\n",
        "\n",
        "  with tf.variable_scope(name) as scope:\n",
        "    kernel = _get_weights_wrapper(\n",
        "      name='weights', shape=shape, weights_decay_factor=0.0, )\n",
        "    output = tf.nn.conv2d(inputs, filter=kernel, strides=strides, padding=padding, name='conv')\n",
        "    if add_bias:\n",
        "      biases = _get_biases_wrapper(name='biases', shape=[shape[-1]] )\n",
        "      output = tf.add(output, biases, name='biasAdd')\n",
        "    if activation_fn is not None:\n",
        "      output = activation_fn(output, name='activation')\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8F7Y8CGbsB4"
      },
      "source": [
        "def _get_weights_wrapper(name, shape, dtype=tf.float32, initializer=initializers.xavier_initializer(),weights_decay_factor=None):\n",
        "  \"\"\"Wrapper over _get_variable_wrapper() to get weights, with weights decay factor in loss.\n",
        "  \"\"\"\n",
        "\n",
        "  weights = _get_variable_wrapper(name=name, shape=shape, dtype=dtype, initializer=initializer)\n",
        "\n",
        "  if weights_decay_factor is not None and weights_decay_factor > 0.0:\n",
        "    weights_wd = tf.multiply(tf.nn.l2_loss(weights), weights_decay_factor, name=name + '/l2loss')\n",
        "    tf.add_to_collection('losses', weights_wd)\n",
        "\n",
        "  return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkO3w5gDcC1b"
      },
      "source": [
        "def _get_biases_wrapper(name, shape, dtype=tf.float32, initializer=tf.constant_initializer(0.0)):\n",
        "  \"\"\"Wrapper over _get_variable_wrapper() to get bias.\n",
        "  \"\"\"\n",
        "  biases = _get_variable_wrapper(name=name, shape=shape, dtype=dtype, initializer=initializer)\n",
        "  return biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GVuHgh2EWw2"
      },
      "source": [
        "def _get_variable_wrapper(\n",
        "  name, shape=None, dtype=None, initializer=None,\n",
        "  regularizer=None,\n",
        "  trainable=True,\n",
        "  collections=None,\n",
        "  caching_device=None,\n",
        "  partitioner=None,\n",
        "  validate_shape=True,\n",
        "  custom_getter=None\n",
        "):\n",
        "  \"\"\"Wrapper over tf.get_variable().\n",
        "  \"\"\"\n",
        "\n",
        "  with tf.device('/cpu:0'):\n",
        "    var = tf.get_variable(\n",
        "      name, shape=shape, dtype=dtype, initializer=initializer,\n",
        "      regularizer=regularizer, trainable=trainable,\n",
        "      collections=collections, caching_device=caching_device,\n",
        "      partitioner=partitioner, validate_shape=validate_shape,\n",
        "      custom_getter=custom_getter\n",
        "    )\n",
        "  return var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBkd3MRuccQh"
      },
      "source": [
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex/K.sum(ex, axis=axis, keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSVP0ymhc6G6"
      },
      "source": [
        "def squash_v1(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
        "    return scale * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQzm6uHwc8Rq"
      },
      "source": [
        "def squash_v0(s, axis=-1, epsilon=1e-7, name=None):\n",
        "    s_squared_norm = K.sum(K.square(s), axis, keepdims=True) + K.epsilon()\n",
        "    safe_norm = K.sqrt(s_squared_norm)\n",
        "    scale = 1 - tf.exp(-safe_norm)\n",
        "    return scale * s / safe_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3v3imL6KwX3"
      },
      "source": [
        "def routing1(u_hat_vecs, beta_a, iterations, output_capsule_num, i_activations, context):\n",
        "    b = keras.backend.zeros_like(u_hat_vecs[:,:,:,0])+ context\n",
        "   \n",
        "    if i_activations is not None:\n",
        "        i_activations = i_activations[...,tf.newaxis]\n",
        "    for i in range(iterations):\n",
        "        if False:\n",
        "            leak = tf.zeros_like(b, optimize=True)\n",
        "            leak = tf.reduce_sum(leak, axis=1, keep_dims=True)\n",
        "            leaky_logits = tf.concat([leak, b], axis=1)\n",
        "            leaky_routing = tf.nn.softmax(leaky_logits, dim=1)        \n",
        "            c = tf.split(leaky_routing, [1, output_capsule_num], axis=1)[1]\n",
        "        else:\n",
        "            c = softmax(b, 1)   \n",
        "#        if i_activations is not None:\n",
        "#            tf.transpose(tf.transpose(c, perm=[0,2,1]) * i_activations, perm=[0,2,1]) \n",
        "        outputs = squash_v1(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
        "        if i < iterations - 1:\n",
        "            b = b + K.batch_dot(outputs, u_hat_vecs, [2, 3])                                    \n",
        "    poses = outputs \n",
        "    activations = K.sqrt(K.sum(K.square(poses), 2))\n",
        "    return poses, activations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObeP2wKDc-Zq"
      },
      "source": [
        "def routing(u_hat_vecs, beta_a, iterations, output_capsule_num, i_activations, context_sensitivity):\n",
        "    b = keras.backend.zeros_like(u_hat_vecs[:,:,:,0]) + context_sensitivity\n",
        "    \n",
        "    if i_activations is not None:\n",
        "        i_activations = i_activations[...,tf.newaxis]\n",
        "    for i in range(iterations):\n",
        "        if False:\n",
        "            leak = tf.zeros_like(b, optimize=True)\n",
        "            leak = tf.reduce_sum(leak, axis=1, keep_dims=True)\n",
        "            leaky_logits = tf.concat([leak, b], axis=1)\n",
        "            leaky_routing = tf.nn.softmax(leaky_logits, dim=1)        \n",
        "            c = tf.split(leaky_routing, [1, output_capsule_num], axis=1)[1]\n",
        "        else:\n",
        "            c = softmax(b, 1)   \n",
        "#        if i_activations is not None:\n",
        "#            tf.transpose(tf.transpose(c, perm=[0,2,1]) * i_activations, perm=[0,2,1]) \n",
        "        outputs = K.batch_dot(c, u_hat_vecs, [2, 2])\n",
        "        if i < iterations - 1:\n",
        "            b = b + K.batch_dot(outputs, u_hat_vecs, [2, 3])                                    \n",
        "    poses = squash_v1(outputs) \n",
        "    activations = K.sqrt(K.sum(K.square(poses), 2))\n",
        "    return poses, activations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSdVM9YCdEMp"
      },
      "source": [
        "def vec_transformationByConv(poses, input_capsule_dim, input_capsule_num, output_capsule_dim, output_capsule_num):                            \n",
        "    kernel = _get_weights_wrapper(\n",
        "      name='weights', shape=[1, input_capsule_dim, output_capsule_dim*output_capsule_num], weights_decay_factor=0.0\n",
        "    )\n",
        "  \n",
        "    u_hat_vecs = keras.backend.conv1d(poses, kernel)\n",
        "    u_hat_vecs = keras.backend.reshape(u_hat_vecs, (-1, input_capsule_num, output_capsule_num, output_capsule_dim))\n",
        "    u_hat_vecs = keras.backend.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "    return u_hat_vecs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-pIavredIK6"
      },
      "source": [
        "def vec_transformationByMat(poses, input_capsule_dim, input_capsule_num, output_capsule_dim, output_capsule_num, shared=False):                        \n",
        "    inputs_poses_shape = poses.get_shape().as_list()\n",
        "    poses = poses[..., tf.newaxis, :]        \n",
        "    poses = tf.tile(poses, [1, 1, output_capsule_num, 1])    \n",
        "    if shared:\n",
        "        kernel = _get_weights_wrapper(name='weights', shape=[1, 1, output_capsule_num, output_capsule_dim, input_capsule_dim], weights_decay_factor=0.0)\n",
        "        kernel = tf.tile(kernel, [inputs_poses_shape[0], input_capsule_num, 1, 1, 1])\n",
        "    else:\n",
        "        kernel = _get_weights_wrapper(name='weights', shape=[1, input_capsule_num, output_capsule_num, output_capsule_dim, input_capsule_dim], weights_decay_factor=0.0)\n",
        "        kernel = tf.tile(kernel, [inputs_poses_shape[0], 1, 1, 1, 1])\n",
        "    \n",
        "    u_hat_vecs = tf.squeeze(tf.matmul(kernel, poses[...,tf.newaxis]),axis=-1)\n",
        "    u_hat_vecs = keras.backend.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "    return u_hat_vecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcvHPFCOdKyX"
      },
      "source": [
        "def capsules_init(inputs, shape, strides, padding, pose_shape, add_bias, name):\n",
        "    with tf.variable_scope(name):   \n",
        "        poses = _conv2d_wrapper(\n",
        "          inputs,\n",
        "          shape=shape[0:-1] + [shape[-1] * pose_shape],\n",
        "          strides=strides,\n",
        "          padding=padding,\n",
        "          add_bias=add_bias,\n",
        "          activation_fn=None,\n",
        "          name='pose_stacked'\n",
        "        )        \n",
        "        poses_shape = poses.get_shape().as_list()    \n",
        "        poses = tf.reshape(poses, [-1, poses_shape[1], poses_shape[2], shape[-1], pose_shape])        \n",
        "        beta_a = _get_weights_wrapper(name='beta_a', shape=[1, shape[-1]])    \n",
        "        poses = squash_v1(poses, axis=-1)  \n",
        "        activations = K.sqrt(K.sum(K.square(poses), axis=-1)) + beta_a        \n",
        "        \n",
        "  \n",
        "\n",
        "    return poses, activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAJ1njh6dOpT"
      },
      "source": [
        "def capsule_fc_layer(nets, output_capsule_num, iterations, name, representation, args):\n",
        "    with tf.variable_scope(name):   \n",
        "        poses, i_activations = nets\n",
        "        input_pose_shape = poses.get_shape().as_list()\n",
        "\n",
        "        u_hat_vecs = vec_transformationByConv(poses,input_pose_shape[-1], input_pose_shape[1],input_pose_shape[-1], output_capsule_num,)\n",
        "        \n",
        "        beta_a = _get_weights_wrapper(name='beta_a', shape=[1, output_capsule_num])\n",
        "\n",
        "        representation = tf.reshape(representation,[input_pose_shape[0],1,1,600])\n",
        "        representation = tf.tile(representation,[1,args.num_classes,1,1])\n",
        "        context = _get_weights_wrapper(name='context', shape=[input_pose_shape[0],args.num_classes,600,input_pose_shape[1]])\n",
        "        context_sensitivity = tf.matmul(representation,context)\n",
        "        context_sensitivity = tf.squeeze(context_sensitivity)\n",
        "\n",
        "        poses, activations = routing(u_hat_vecs, beta_a, iterations, output_capsule_num, i_activations, context_sensitivity)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    return poses, activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE-SPLBwdR2a"
      },
      "source": [
        "def capsule_flatten(nets):\n",
        "    poses, activations = nets\n",
        "    input_pose_shape = poses.get_shape().as_list()\n",
        "    \n",
        "    poses = tf.reshape(poses, [\n",
        "                    -1, input_pose_shape[1]*input_pose_shape[2]*input_pose_shape[3], input_pose_shape[-1]]) \n",
        "    activations = tf.reshape(activations, [\n",
        "                    -1, input_pose_shape[1]*input_pose_shape[2]*input_pose_shape[3]])\n",
        "    \n",
        "\n",
        "    return poses, activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uMUkhf1dVJy"
      },
      "source": [
        "def capsule_conv_layer(nets, shape, strides, iterations, name, representation):   \n",
        "    with tf.variable_scope(name):              \n",
        "        poses, i_activations = nets\n",
        "        \n",
        "        inputs_poses_shape = poses.get_shape().as_list()\n",
        "\n",
        "        hk_offsets = [\n",
        "          [(h_offset + k_offset) for k_offset in range(0, shape[0])] for h_offset in\n",
        "          range(0, inputs_poses_shape[1] + 1 - shape[0], strides[1])\n",
        "        ]\n",
        "        wk_offsets = [\n",
        "          [(w_offset + k_offset) for k_offset in range(0, shape[1])] for w_offset in\n",
        "          range(0, inputs_poses_shape[2] + 1 - shape[1], strides[2])\n",
        "        ]\n",
        "    \n",
        "        inputs_poses_patches = tf.transpose(\n",
        "          tf.gather(\n",
        "            tf.gather(\n",
        "              poses, hk_offsets, axis=1, name='gather_poses_height_kernel'\n",
        "            ), wk_offsets, axis=3, name='gather_poses_width_kernel'\n",
        "          ), perm=[0, 1, 3, 2, 4, 5, 6], name='inputs_poses_patches'\n",
        "        )\n",
        "        \n",
        "    \n",
        "        inputs_poses_shape = inputs_poses_patches.get_shape().as_list()\n",
        "        inputs_poses_patches = tf.reshape(inputs_poses_patches, [\n",
        "                                -1, shape[0]*shape[1]*shape[2], inputs_poses_shape[-1]\n",
        "                                ])\n",
        "\n",
        "        i_activations_patches = tf.transpose(\n",
        "          tf.gather(\n",
        "            tf.gather(\n",
        "              i_activations, hk_offsets, axis=1, name='gather_activations_height_kernel'\n",
        "            ), wk_offsets, axis=3, name='gather_activations_width_kernel'\n",
        "          ), perm=[0, 1, 3, 2, 4, 5], name='inputs_activations_patches'\n",
        "        )\n",
        "        patches_dim = i_activations_patches.get_shape().as_list()\n",
        "        \n",
        "        i_activations_patches = tf.reshape(i_activations_patches, [\n",
        "                                -1, shape[0]*shape[1]*shape[2]]\n",
        "                                )\n",
        "        u_hat_vecs = vec_transformationByConv(\n",
        "                  inputs_poses_patches,\n",
        "                  inputs_poses_shape[-1], shape[0]*shape[1]*shape[2],\n",
        "                  inputs_poses_shape[-1], shape[3],\n",
        "                  )\n",
        "       \n",
        "\n",
        "        # patches_dim = i_activations_patches.get_shape().as_list()\n",
        "        representation = tf.expand_dims(representation, -1)\n",
        "        representation = tf.tile(representation,[1,1,8])\n",
        "        representation = tf.reshape(representation,[-1,1,16,300])\n",
        "        representation = tf.tile(representation,[1,patches_dim[1],1,1])\n",
        "        representation = tf.reshape(representation,[-1,16,300])\n",
        "        context = _get_weights_wrapper(name='context', shape=[patches_dim[1]*patches_dim[0],300,48])\n",
        "        context_sensitivity = tf.matmul(representation,context)\n",
        "    \n",
        "        beta_a = _get_weights_wrapper(\n",
        "                name='beta_a', shape=[1, shape[3]]\n",
        "                )\n",
        "        poses, activations = routing(u_hat_vecs, beta_a, iterations, shape[3], i_activations_patches, context_sensitivity)\n",
        "        poses = tf.reshape(poses, [\n",
        "                    inputs_poses_shape[0], inputs_poses_shape[1],\n",
        "                    inputs_poses_shape[2], shape[3],\n",
        "                    inputs_poses_shape[-1]]\n",
        "                ) \n",
        "        activations = tf.reshape(activations, [\n",
        "                    inputs_poses_shape[0],inputs_poses_shape[1],\n",
        "                    inputs_poses_shape[2],shape[3]]\n",
        "                ) \n",
        "        nets = poses, activations            \n",
        "   \n",
        "    return nets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1iFKGQvgGNm"
      },
      "source": [
        "# **Load Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF_9G7SXaK2C"
      },
      "source": [
        "## From other repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5wyzyj6ZcXL"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def checkdirs(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def read_text(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        best_epoch = int(f.read())\n",
        "        return best_epoch\n",
        "\n",
        "\n",
        "def write_text(text, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(text)\n",
        "\n",
        "\n",
        "def load_pickle_data(pickle_dir, dataset):\n",
        "    assert os.path.exists(pickle_dir)\n",
        "    pickle_data = pickle.load(open(pickle_dir+dataset, \"rb\"))\n",
        "    return pickle_data\n",
        "\n",
        "\n",
        "def get_idx_from_sent(sent, word_idx_map, max_length):\n",
        "    x = []\n",
        "    words = sent.split()[:max_length]\n",
        "    for word in words:\n",
        "        if word in word_idx_map:\n",
        "            x.append(word_idx_map[word])\n",
        "    while len(x) < max_length:\n",
        "        x.append(0)\n",
        "    return x\n",
        "\n",
        "\n",
        "def make_idx_data(raw_datas, word_idx_map, len_train, max_length):\n",
        "    data = []\n",
        "    for raw_data in raw_datas:\n",
        "        sent = get_idx_from_sent(raw_data[\"text\"], word_idx_map, max_length)\n",
        "        sent.append(raw_data[\"y\"])\n",
        "        data.append(sent)\n",
        "    split = len_train\n",
        "    train = np.array(data[:split], dtype=\"int\")\n",
        "    test = np.array(data[split:], dtype=\"int\")\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def preprocessing(data_path, dataset, long_sent=800):\n",
        "    \"\"\"\n",
        "\n",
        "    :param data_path: base directory\n",
        "    :param dataset: select dataset {'20news', 'mr', 'trec', 'mpqa'}\n",
        "    :param long_sent: if dataset has long sentences, set to be constant length value\n",
        "    :return: seq_length, num_classes, vocab_size, x_train, y_train, x_test, y_test, pre-train_word (GloVe 840b),\n",
        "    word_idx\n",
        "    \"\"\"\n",
        "\n",
        "    assert os.path.exists(data_path) is True\n",
        "\n",
        "    x = load_pickle_data(data_path, dataset)\n",
        "    data_frame, pretrain_word, len_train, n_exist_word, vocab, word_idx = x\n",
        "\n",
        "    max_l = int(np.max(pd.DataFrame(data_frame)[\"num_words\"]))\n",
        "\n",
        "    if dataset in [\"reuters\", \"20news\", \"imdb\", 'mr']:\n",
        "        train, test = make_idx_data(data_frame, word_idx, len_train, long_sent)\n",
        "    else:\n",
        "        train, test = make_idx_data(data_frame, word_idx, len_train, max_l)\n",
        "\n",
        "    # train[:, :-1] = word idx\n",
        "    # train[:, -1] = true label\n",
        "    x_train = train[:, :-1]\n",
        "    y_train = train[:, -1]\n",
        "\n",
        "    x_test = test[:, :-1]\n",
        "    y_test = test[:, -1]\n",
        "    sequence_length = len(x_train[0])\n",
        "\n",
        "    # make one-hot\n",
        "    labels = sorted(list(set(y_train)))\n",
        "    one_hot = np.zeros((len(labels), len(labels)), int)\n",
        "    np.fill_diagonal(one_hot, 1)\n",
        "    label_dict = dict(zip(labels, one_hot))\n",
        "\n",
        "    y_train = np.eye(len(label_dict))[y_train]\n",
        "    num_class = y_train.shape[1]\n",
        "\n",
        "    y_test = np.eye(len(label_dict))[y_test]\n",
        "    vocab_size = pretrain_word.shape[0]\n",
        "\n",
        "    print(\"sequence length :\", sequence_length)\n",
        "    print(\"vocab size :\", vocab_size)\n",
        "    print(\"num classes :\", num_class)\n",
        "\n",
        "    return sequence_length, num_class, vocab_size, x_train, y_train, x_test, y_test, pretrain_word, word_idx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2VxM66xaOcx"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Dh3LxZYJee"
      },
      "source": [
        "def text_preprocessing(train_data,test_data):\n",
        "  train_data_texts = train_data['comment']\n",
        "  train_data_labels = train_data['label']\n",
        "  test_data_texts = test_data['comment']\n",
        "  test_data_labels = test_data['label']\n",
        "\n",
        "\n",
        "  comment_texts = []\n",
        "  comment_labels = []\n",
        "\n",
        "  train_text = []\n",
        "  test_text = []\n",
        "  train_labels=[]\n",
        "  test_labels=[]\n",
        "\n",
        "  for label in train_data_labels:\n",
        "    if label == \"POSITIVE\":\n",
        "      train_labels.append(1)\n",
        "    else:\n",
        "      train_labels.append(0)\n",
        "  comment_labels.append(train_labels)\n",
        "\n",
        "  for label in test_data_labels:\n",
        "    if label == \"POSITIVE\":\n",
        "      test_labels.append(1)\n",
        "    else:\n",
        "      test_labels.append(0)\n",
        "  comment_labels.append(test_labels)\n",
        "  \n",
        "\n",
        "  for comment in train_data_texts:\n",
        "    lines = []\n",
        "    try:\n",
        "      words = comment.split()\n",
        "      lines += words\n",
        "    except:\n",
        "      continue\n",
        "    train_text.append(lines)\n",
        "  comment_texts.append(train_text)\n",
        "\n",
        "  for comment in test_data_texts:\n",
        "    lines = []\n",
        "    try:\n",
        "      words = comment.split()\n",
        "      lines += words\n",
        "    except:\n",
        "      continue\n",
        "    test_text.append(lines)\n",
        "  comment_texts.append(test_text)\n",
        "\n",
        "\n",
        "  return comment_texts,comment_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYpPdjNOX1SF"
      },
      "source": [
        "EMBEDDING_SIZE = 300 #@param [50, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "embedding_type = \"fasttext\" #@param [\"fasttext\",\"word2vec\"]\n",
        "experiment_no = \"2000\" #@param [] {allow-input: true}\n",
        "\n",
        "model_name = \"DS_Caps_sinhala\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvNsTxkqcO6z"
      },
      "source": [
        "folder_path =  '/content/drive/My Drive/Final Year Project/FYP/Sentiment Analysis/Implementation/'\n",
        "\n",
        "lankadeepa_data_path = folder_path + 'corpus/new/preprocess_from_isuru/lankadeepa_tagged_comments.csv'\n",
        "gossip_lanka_data_path = folder_path + 'corpus/new/preprocess_from_isuru/gossip_lanka_tagged_comments.csv'\n",
        "# \"corpus/new/preprocess_from_unicode_values/lankadeepa_tagged_comments.csv\"\n",
        "# \"corpus/new/preprocess_from_unicode_values/gossip_lanka_tagged_comments.csv\"\n",
        "\n",
        "context = 5\n",
        "embeds = \"fasttext\"\n",
        "word_embedding_path = folder_path + \"word_embedding/\"+embeds+\"/source2_data_from_gosspiLanka_and_lankadeepa/\"+str(EMBEDDING_SIZE)+\"/\"+embedding_type+\"_\"+str(EMBEDDING_SIZE)+\"_\"+str(context)\n",
        "word_embedding_keydvectors_path = folder_path + \"word_embedding/\"+embeds+\"/source2_data_from_gosspiLanka_and_lankadeepa/\"+str(EMBEDDING_SIZE)+\"/keyed_vectors/keyed.kv\"\n",
        "embedding_matrix_path = folder_path + 'Sentiment Analysis/CNN RNN/embedding_matrix/'+embedding_type+'_lankadeepa_gossiplanka_'+str(EMBEDDING_SIZE)+'_'+str(context)\n",
        "\n",
        "experiment_name = folder_path + \"Sentiment Analysis/CNN RNN/experiments/\" +str(experiment_no) + \"_\"+ model_name +\"_\"+embedding_type+\"_\"+str(EMBEDDING_SIZE)+\"_\"+str(context)\n",
        "model_save_path = folder_path + \"Sentiment Analysis/CNN RNN/saved_models/\"+str(experiment_no)+\"_weights_best_\"+model_name+\"_\"+embedding_type+\"_\"+str(experiment_no)+\".hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XAcr0JlbFKM"
      },
      "source": [
        "lankadeepa_data = pd.read_csv(lankadeepa_data_path)[:9059]\n",
        "gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n",
        "gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])\n",
        "\n",
        "all_data = pd.concat([lankadeepa_data,gossipLanka_data], ignore_index=True)\n",
        "all_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk7iSweShnqA"
      },
      "source": [
        "all_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USRX93-thc6c"
      },
      "source": [
        "def text_preprocessing(data):\n",
        "  comments = data['comment']\n",
        "  labels = data['label']\n",
        "\n",
        "  comments_splitted = []\n",
        "\n",
        "  for comment in comments:\n",
        "    lines = []\n",
        "    try:\n",
        "      words = comment.split()\n",
        "      lines += words\n",
        "    except:\n",
        "      continue\n",
        "    comments_splitted.append(lines)\n",
        "\n",
        "  return comments_splitted,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KijmNckHv_zz"
      },
      "source": [
        "comment_texts, comment_labels = text_preprocessing(all_data)\n",
        "\n",
        "# prepare tokenizer\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(comment_texts)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwzAfg64DEBl"
      },
      "source": [
        "comment_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjfooStlB97b"
      },
      "source": [
        "for i in range(len(comment_labels)):\n",
        "  comment_labels[i] = comment_labels[i]-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeGpOpgf8Bwq"
      },
      "source": [
        "comment_labels[15054]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzmS15u-wCP_"
      },
      "source": [
        "encoded_docs = t.texts_to_sequences(comment_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bso9LkD2wFUf"
      },
      "source": [
        "# max_length = len(max(encoded_docs, key=len))\n",
        "max_length = 30\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length,padding='post')\n",
        "\n",
        "comment_labels = np.array(comment_labels)\n",
        "padded_docs = np.array(padded_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu18iULjwH8W"
      },
      "source": [
        "# comment_labels = pd.get_dummies(comment_labels).values\n",
        "print('Shape of label tensor:', comment_labels.shape)\n",
        "comment_labels = pd.get_dummies(comment_labels).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwoHwrCswKHc"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(padded_docs, comment_labels, test_size=0.1, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__J2ndx9vFa"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlrFId49wODu"
      },
      "source": [
        "(unique, counts) = np.unique(y_test, return_counts = True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print(frequencies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvav4IQqwRa_"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X79YOIhwUW6"
      },
      "source": [
        "# !pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5-EiWkXwbIm"
      },
      "source": [
        "# import gensim\n",
        "# from gensim.models.keyedvectors import KeyedVectors\n",
        "# from gensim.models.fasttext import FastText\n",
        "# from gensim.models import word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPX0J7ONwdGb"
      },
      "source": [
        "# word_embedding_model = FastText.load(word_embedding_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nsHwRhRwfQ7"
      },
      "source": [
        "# word_vectors = word_embedding_model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39YHfhJDwgyI"
      },
      "source": [
        "# word_vectors.vocab.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO6bJ4N9wjCq"
      },
      "source": [
        "# def generate_embedding_matrix():\n",
        "#   if (embedding_type == 'fasText'):\n",
        "#     word_embedding_model = FastText.load(word_embedding_path)\n",
        "#   else:\n",
        "#     word_embedding_model = word2vec.Word2Vec.load(word_embedding_path)\n",
        "    \n",
        "#   word_vectors = word_embedding_model.wv\n",
        "#   word_vectors.save(word_embedding_keydvectors_path)\n",
        "#   word_vectors = KeyedVectors.load(word_embedding_keydvectors_path, mmap='r')\n",
        "\n",
        "#   embeddings_index = dict()\n",
        "#   for word, vocab_obj in word_vectors.vocab.items():\n",
        "#     embeddings_index[word]=word_vectors[word]\n",
        "\n",
        "#   # create a weight matrix for words in training docs\n",
        "#   embedding_matrix = zeros((vocab_size, EMBEDDING_SIZE))\n",
        "#   for word, i in t.word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#       embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#   # pickle.dump(embedding_matrix, open(embedding_matrix_path, 'wb'))\n",
        "#   return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de77NTvNworH"
      },
      "source": [
        "def load_word_embedding_matrix():\n",
        "  f = open(embedding_matrix_path, 'rb')\n",
        "  embedding_matrix= np.array(pickle.load(f))\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7upX8V8Xwqoj"
      },
      "source": [
        "embedding_matrix = load_word_embedding_matrix()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PARyB28Vi9VI"
      },
      "source": [
        "# **Loss functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkAyzy_3hrYf"
      },
      "source": [
        "def spread_loss(labels, activations, margin):\n",
        "    activations_shape = activations.get_shape().as_list()\n",
        "    mask_t = tf.equal(labels, 1)\n",
        "    mask_i = tf.equal(labels, 0)    \n",
        "    activations_t = tf.reshape(\n",
        "      tf.boolean_mask(activations, mask_t), [activations_shape[0], 1]\n",
        "    )    \n",
        "    activations_i = tf.reshape(\n",
        "      tf.boolean_mask(activations, mask_i), [activations_shape[0], activations_shape[1] - 1]\n",
        "    )    \n",
        "    gap_mit = tf.reduce_sum(tf.square(tf.nn.relu(margin - (activations_t - activations_i))))\n",
        "    return gap_mit        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2UfkOrSjFvX"
      },
      "source": [
        "def cross_entropy(y, preds):    \n",
        "    y = tf.argmax(y, axis=1)\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=preds, labels=y)                                               \n",
        "    loss = tf.reduce_mean(loss) \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEI1tgTnjUg_"
      },
      "source": [
        "def margin_loss(y, preds):    \n",
        "    y = tf.cast(y,tf.float32)\n",
        "    loss = y * tf.square(tf.maximum(0., 0.9 - preds)) + \\\n",
        "        0.25 * (1.0 - y) * tf.square(tf.maximum(0., preds - 0.1))\n",
        "    loss = tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
        "#    loss = tf.reduce_mean(loss)    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jam_fKSJjYfg"
      },
      "source": [
        "def capsule_model_A(X, num_classes, args, context, context_sensitivity1):\n",
        "    with tf.variable_scope('capsule_'+str(3), reuse=tf.AUTO_REUSE):   \n",
        "        nets = _conv2d_wrapper(\n",
        "                X, shape=[3, 300, 1, 32], strides=[1, 2, 1, 1], padding='VALID', \n",
        "                add_bias=True, activation_fn=tf.nn.relu, name='conv1'\n",
        "            )\n",
        "      \n",
        "        nets = capsules_init(nets, shape=[1, 1, 32, 16], strides=[1, 1, 1, 1], \n",
        "                             padding='VALID', pose_shape=16, add_bias=True, name='primary')                        \n",
        "        nets = capsule_conv_layer(nets, shape=[3, 1, 16, 16], strides=[1, 1, 1, 1], iterations=3, name='conv2', representation=context_sensitivity1)\n",
        "        nets = capsule_flatten(nets)\n",
        "        poses, activations = capsule_fc_layer(nets, num_classes, 3, 'fc2', context, args) \n",
        "    return poses, activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbqUEzcbotXD"
      },
      "source": [
        "def capsule_model_B(X, num_classes, args, representation1, representation2):\n",
        "    poses_list = []\n",
        "    for _, ngram in enumerate([3,4,5]):\n",
        "        with tf.variable_scope('capsule_'+str(ngram), reuse=tf.AUTO_REUSE): \n",
        "            nets = _conv2d_wrapper(\n",
        "                X, shape=[ngram, 300, 1, 32], strides=[1, 2, 1, 1], padding='VALID', \n",
        "                add_bias=True, activation_fn=tf.nn.relu, name='conv1'\n",
        "            )\n",
        "         \n",
        "            nets = capsules_init(nets, shape=[1, 1, 32, 16], strides=[1, 1, 1, 1], \n",
        "                                 padding='VALID', pose_shape=16, add_bias=True, name='primary')                        \n",
        "            nets = capsule_conv_layer(nets, shape=[3, 1, 16, 16], strides=[1, 1, 1, 1], iterations=3, name='conv2', representation=representation1)\n",
        "            nets = capsule_flatten(nets)\n",
        "            poses, activations = capsule_fc_layer(nets, num_classes, 3, 'fc2', representation2, args)\n",
        "            poses_list.append(poses)\n",
        "    \n",
        "    poses = tf.reduce_mean(tf.convert_to_tensor(poses_list), axis=0) \n",
        "    activations = K.sqrt(K.sum(K.square(poses), 2))\n",
        "    return poses, activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz6aicUlku28"
      },
      "source": [
        "# Set Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-PH6uUSc9NH"
      },
      "source": [
        "## Config Capsule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un6p6OLXkPnt"
      },
      "source": [
        "class Args:\n",
        "  embedding_type = \"static\"\n",
        "  dataset = \"reuters_multilabel_dataset\"\n",
        "  loss_type = \"margin_loss\"\n",
        "  model_type = \"capsule-B\"\n",
        "  has_test = 1\n",
        "  has_dev = 1\n",
        "  num_epochs = 50\n",
        "  batch_size = 4\n",
        "  use_orphan = True\n",
        "  use_leaky = True\n",
        "  learning_rate = 0.001\n",
        "  margin = 0.2\n",
        "  num_classes = 4\n",
        "  vocab_size = vocab_size\n",
        "  vec_size = 300\n",
        "  max_sent = max_length\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cA5u4HrkuCb"
      },
      "source": [
        "args = Args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-tGsjK7h2ow"
      },
      "source": [
        "## Config SLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_G8jss9k7iO"
      },
      "source": [
        "class Config(object):\n",
        "    vocab_size=vocab_size\n",
        "    max_grad_norm = 5\n",
        "    init_scale = 0.05\n",
        "    hidden_size = 300\n",
        "    lr_decay = 0.95\n",
        "    valid_portion = 0.0\n",
        "    batch_size = 16\n",
        "    keep_prob = 0.8\n",
        "    #0.05\n",
        "    learning_rate = 0.0001\n",
        "    max_epoch =2\n",
        "    # max_max_epoch =40\n",
        "    max_max_epoch = 15\n",
        "    num_label=5\n",
        "    attention_iteration=3\n",
        "    random_initialize=True\n",
        "    embedding_trainable=True\n",
        "    l2_beta=0.0\n",
        "    layer = 4\n",
        "    step = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKbA7mJlk_P9"
      },
      "source": [
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7puLY3_eh5Xk"
      },
      "source": [
        "# Tensorflow Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbw0m6hF6Eu6"
      },
      "source": [
        "# X = tf.placeholder(tf.int32, [args.batch_size, args.max_sent], name=\"input_x\")\n",
        "# y = tf.placeholder(tf.int64, [args.batch_size, args.num_classes], name=\"input_y\")\n",
        "# is_training = tf.placeholder_with_default(False, shape=())    \n",
        "# learning_rate = tf.placeholder(dtype='float32') \n",
        "# margin = tf.placeholder(shape=(),dtype='float32') \n",
        "\n",
        "# l2_loss = tf.constant(0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQsv_kInxzBk"
      },
      "source": [
        "# print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvTrMdOD6S2X"
      },
      "source": [
        "# w2v = np.array(embedding_matrix,dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U6MuF3zxzBo"
      },
      "source": [
        "# w2v.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMzPsJRTDew-"
      },
      "source": [
        "# W1 = tf.Variable(w2v, trainable = False)\n",
        "# X_embedding = tf.nn.embedding_lookup(W1, X)\n",
        "# X_embedding = X_embedding[...,tf.newaxis] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phPdA_-sDwRs"
      },
      "source": [
        "# tf.logging.info(\"input dimension:{}\".format(X_embedding.get_shape()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yy4OqEcFBvK"
      },
      "source": [
        "# X_embedding.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcSg8qUeeQCr"
      },
      "source": [
        "# classifier = Classifer(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnbfMz5Vp6eI"
      },
      "source": [
        "# hidden_states,representation, representation1 = classifier.slstm_basic_layer(X_embedding, config, [max_length]*(args.batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWe0PbJTDoje"
      },
      "source": [
        "# hidden_states.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EHJcIaTXmoX"
      },
      "source": [
        "# representation1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6-iRB4sn_dt"
      },
      "source": [
        "# representation1 = tf.expand_dims(representation1, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ4jPhgTXmWm"
      },
      "source": [
        "# representation1 = tf.tile(representation1,[1,1,2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_Jt65IeZEGV"
      },
      "source": [
        "# representation1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A99vdBc2ZIUC"
      },
      "source": [
        "# representation1 = tf.reshape(representation1,[-1,1,16,75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AZBPqwyZISn"
      },
      "source": [
        "# representation1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EE8HqVDpizq"
      },
      "source": [
        "# representation1 = tf.tile(representation1,[1,23,1,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdLFMpaPppeS"
      },
      "source": [
        "# representation1.shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV7rSLC1ppj2"
      },
      "source": [
        "# representation1 = tf.reshape(representation1,[-1,16,75])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAxzBMKtp5d3"
      },
      "source": [
        "# representation1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TICN5ZfZesW"
      },
      "source": [
        "# context1 = tf.Variable(tf.random_normal([92,75,48], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVacLmI2Zj_2"
      },
      "source": [
        "# context_sensitivity1 = tf.matmul(representation1,context1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ggHQBpadzNZ"
      },
      "source": [
        "# context_sensitivity1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8OvPL3_dzVP"
      },
      "source": [
        "# context_sensitivity1 = tf.reshape(context_sensitivity1,[92,16,48])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqopHDnfeEom"
      },
      "source": [
        "# context_sensitivity1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuHJTHdIJy2B"
      },
      "source": [
        "# hidden_states = tf.expand_dims(hidden_states, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LufI5RtbJ9lG"
      },
      "source": [
        "# hidden_states.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vT-jdP5_wau"
      },
      "source": [
        "# hidden_states.shape[0:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu8F_C_2G6AT"
      },
      "source": [
        "# representation = tf.reshape(representation,[4,2,300,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lATefnbH0oE"
      },
      "source": [
        "# representation.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn5s1T97IdLb"
      },
      "source": [
        "# context = tf.Variable(tf.random_normal([4,2,368,300], mean=0.0, stddev=0.1, dtype=tf.float32), dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ3-1B6fIyMp"
      },
      "source": [
        "# context_sensitivity = tf.matmul(context,representation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMFdIekXJAd3"
      },
      "source": [
        "# context_sensitivity = tf.squeeze(context_sensitivity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwqtM-gtJUkf"
      },
      "source": [
        "# context_sensitivity.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aMCYXz5D5U0"
      },
      "source": [
        "# poses, activations = capsule_model_A(X_embedding, args.num_classes)   \n",
        "# poses, activations = capsule_model_B(hidden_states, args.num_classes, args, representation, representation1)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1t6-oPRp-mr"
      },
      "source": [
        "# loss = margin_loss(y, activations) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn1wQzFwyyRN"
      },
      "source": [
        "# y_pred = tf.argmax(activations, axis=1, name=\"y_proba\")    \n",
        "# correct = tf.equal(tf.argmax(y, axis=1), y_pred, name=\"correct\")\n",
        "# accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)   \n",
        "# training_op = optimizer.minimize(loss, name=\"training_op\")\n",
        "# gradients, variables = zip(*optimizer.compute_gradients(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVRu8KNo3GcR"
      },
      "source": [
        "# with tf.device('/cpu:0'):\n",
        "#     global_step = tf.train.get_or_create_global_step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wagiiKLp3Iap"
      },
      "source": [
        "args.max_sent = max_length\n",
        "threshold = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1tiaE3Xy7Av"
      },
      "source": [
        "# grad_check = [tf.check_numerics(g, message='Gradient NaN Found!')\n",
        "#               for g in gradients if g is not None] + [tf.check_numerics(loss, message='Loss NaN Found')]\n",
        "# with tf.control_dependencies(grad_check):\n",
        "#     training_op = optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO5TpkGKzMw3"
      },
      "source": [
        "# sess = tf.InteractiveSession()\n",
        "# from keras import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFGsOcj50Eov"
      },
      "source": [
        "class BatchGenerator(object):\n",
        "    \"\"\"Generate and hold batches.\"\"\"\n",
        "    def __init__(self, dataset,label, batch_size,input_size, is_shuffle=True):\n",
        "      self._dataset = dataset\n",
        "      self._label = label\n",
        "      self._batch_size = batch_size    \n",
        "      self._cursor = 0      \n",
        "      self._input_size = input_size      \n",
        "      \n",
        "      if is_shuffle:\n",
        "          index = np.arange(len(self._dataset))\n",
        "          np.random.shuffle(index)\n",
        "          self._dataset = np.array(self._dataset)[index]\n",
        "          self._label = np.array(self._label)[index]\n",
        "      else:\n",
        "          self._dataset = np.array(self._dataset)\n",
        "          self._label = np.array(self._label)\n",
        "    def next(self):\n",
        "      if self._cursor + self._batch_size > len(self._dataset):\n",
        "          self._cursor = 0\n",
        "      \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"      \n",
        "      batch_x = self._dataset[self._cursor : self._cursor + self._batch_size,:]\n",
        "      batch_y = self._label[self._cursor : self._cursor + self._batch_size]\n",
        "      self._cursor += self._batch_size\n",
        "      return batch_x, batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVlmXymbzaFP"
      },
      "source": [
        "# n_iterations_per_epoch = len(X_train) // args.batch_size\n",
        "# n_iterations_test = len(X_test) // args.batch_size\n",
        "# n_iterations_dev = len(X_test) // args.batch_size    \n",
        "\n",
        "# mr_train = BatchGenerator(X_train,y_train, args.batch_size, 0)    \n",
        "# mr_dev = BatchGenerator(X_test,y_test, args.batch_size, 0)\n",
        "# mr_test = BatchGenerator(X_test,y_test, args.batch_size, 0, is_shuffle=False)\n",
        "\n",
        "best_model = None\n",
        "best_epoch = 0\n",
        "best_acc_val = 0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd1LUgSK0cKv"
      },
      "source": [
        "# init = tf.global_variables_initializer()\n",
        "# sess.run(init)     \n",
        "\n",
        "lr = args.learning_rate\n",
        "m = args.margin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrGM-TaP-Ti0"
      },
      "source": [
        "# array1 = [2,3,0,1]\n",
        "# z = utils.to_categorical(array1, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBBT8uzA_TON"
      },
      "source": [
        "# z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7YA8eGKiHf6"
      },
      "source": [
        "# Train/ Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhSAzGNu0iJ_"
      },
      "source": [
        "# loss_vals, acc_vals = [], []\n",
        "# for epoch in range(args.num_epochs):\n",
        "#     for iteration in range(1, n_iterations_per_epoch + 1):                \n",
        "#         X_batch, y_batch = mr_train.next()     \n",
        "#         y_batch = utils.to_categorical(y_batch, args.num_classes)        \n",
        "#         _, loss_train, probs, capsule_pose = sess.run(\n",
        "#             [training_op, loss, activations, poses],\n",
        "#             feed_dict={X: X_batch[:,:args.max_sent],\n",
        "#                        y: y_batch,\n",
        "#                        is_training: True,\n",
        "#                        learning_rate:lr,\n",
        "#                        margin:m})        \n",
        "#         print(\"\\rIteration: ({:.1f}%)  Loss: {:.5f}\".format(\n",
        "#                   iteration * 100 / n_iterations_per_epoch,\n",
        "#                   loss_train),\n",
        "#               end=\"\")                        \n",
        "#     loss_vals, acc_vals = [], []\n",
        "#     for iteration in range(1, n_iterations_dev + 1):\n",
        "#         X_batch, y_batch = mr_dev.next()            \n",
        "#         y_batch = utils.to_categorical(y_batch, args.num_classes)\n",
        "#         loss_val, acc_val = sess.run(\n",
        "#                 [loss, accuracy],\n",
        "#                 feed_dict={X: X_batch[:,:args.max_sent],\n",
        "#                            y: y_batch,\n",
        "#                            is_training: False,\n",
        "#                            margin:m})\n",
        "#         loss_vals.append(loss_val)\n",
        "#         acc_vals.append(acc_val)\n",
        "#     loss_val, acc_val = np.mean(loss_vals), np.mean(acc_vals)    \n",
        "#     print(\"\\rEpoch: {}  Val accuracy: {:.1f}%  Loss: {:.4f}\".format(\n",
        "#         epoch + 1, acc_val * 100, loss_val))\n",
        "               \n",
        "#     # preds_list, y_list = [], []\n",
        "#     # for iteration in range(1, n_iterations_test + 1):\n",
        "#     #     X_batch, y_batch = mr_test.next()             \n",
        "#     #     probs = sess.run([activations],\n",
        "#     #             feed_dict={X:X_batch[:,:args.max_sent],\n",
        "#     #                        is_training: False})\n",
        "#     #     preds_list = preds_list + probs[0].tolist()\n",
        "#     #     y_list = y_list + y_batch.tolist()\n",
        "        \n",
        "#     # y_list = np.array(y_list)\n",
        "#     # preds_probs = np.array(preds_list)                \n",
        "#     # preds_probs[np.where( preds_probs >= threshold )] = 1.0\n",
        "#     # preds_probs[np.where( preds_probs < threshold )] = 0.0 \n",
        "#     # print(y_list)\n",
        "#     # print(preds_probs)\n",
        "    \n",
        "#     # #[precision, recall, F1, support] = precision_recall_fscore_support(y_list, preds_probs, average='samples')\n",
        "#     # acc = accuracy_score(y_list, preds_probs)\n",
        "\n",
        "#     # print ('\\rER: %.3f' % acc, 'Precision: %.3f' % precision, 'Recall: %.3f' % recall, 'F1: %.3f' % F1)  \n",
        "#     # # if args.model_type == 'CNN' or args.model_type == 'KIMCNN':\n",
        "#     # #     lr = max(1e-6, lr * 0.8)\n",
        "#     if args.loss_type == 'margin_loss':    \n",
        "#         m = min(0.9, m + 0.1)\n",
        "# print(acc_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm6pNJohqoKK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FrsLcuXoD0N"
      },
      "source": [
        "acc_per_fold = []\n",
        "precision_per_fold = []\n",
        "recall_per_fold = []\n",
        "f1_per_fold = []\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "fold_no = 1\n",
        "inputs = padded_docs\n",
        "targets = comment_labels\n",
        "\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "  \n",
        "\n",
        "    n_iterations_per_epoch = len(inputs[train]) // args.batch_size\n",
        "    n_iterations_test = len(inputs[test]) // args.batch_size\n",
        "\n",
        "    mr_train1 = BatchGenerator(inputs[train], targets[train], args.batch_size, 0)    \n",
        "    mr_test1 = BatchGenerator(inputs[test], targets[test], args.batch_size, 0, is_shuffle=False)\n",
        "    best_accuracy = 0.\n",
        "    best_precision = 0.\n",
        "    best_recall = 0.\n",
        "    best_f1 = 0.\n",
        "\n",
        "    X = tf.placeholder(tf.int32, [args.batch_size, args.max_sent], name=\"input_x\")\n",
        "    y = tf.placeholder(tf.int64, [args.batch_size, args.num_classes], name=\"input_y\")\n",
        "    is_training = tf.placeholder_with_default(False, shape=())    \n",
        "    learning_rate = tf.placeholder(dtype='float32') \n",
        "    margin = tf.placeholder(shape=(),dtype='float32') \n",
        "\n",
        "    l2_loss = tf.constant(0.0)\n",
        "    w2v = np.array(embedding_matrix,dtype=np.float32)\n",
        "\n",
        "    W1 = tf.Variable(w2v, trainable = False)\n",
        "    X_embedding = tf.nn.embedding_lookup(W1, X)\n",
        "    X_embedding = X_embedding[...,tf.newaxis] \n",
        "\n",
        "    classifier = Classifer(config)\n",
        "    hidden_states,representation, representation1 = classifier.slstm_basic_layer(X_embedding, config, [max_length]*(args.batch_size))\n",
        "    hidden_states = tf.expand_dims(hidden_states, -1)\n",
        "\n",
        "\n",
        "    poses, activations = capsule_model_A(hidden_states, args.num_classes, args, representation, representation1)   \n",
        "    loss = margin_loss(y, activations) \n",
        "    y_pred = tf.argmax(activations, axis=1, name=\"y_proba\")    \n",
        "    correct = tf.equal(tf.argmax(y, axis=1), y_pred, name=\"correct\")\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name = 'opt'+str(fold_no))   \n",
        "    training_op = optimizer.minimize(loss, name=\"training_op\")\n",
        "    gradients, variables = zip(*optimizer.compute_gradients(loss)) \n",
        "    with tf.Session() as sess:\n",
        "\n",
        "      init = tf.global_variables_initializer()\n",
        "      sess.run(init)\n",
        "\n",
        "      for epoch in range(0,6): \n",
        "\n",
        "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
        "            \n",
        "          \n",
        "          X_batch, y_batch = mr_train1.next()          \n",
        "          _, loss_train, probs, capsule_pose = sess.run(\n",
        "                [training_op, loss, activations, poses],\n",
        "                feed_dict={X: X_batch[:,:args.max_sent],\n",
        "                          y: y_batch,\n",
        "                          is_training: True,\n",
        "                          learning_rate:lr,\n",
        "                          margin:m})        \n",
        "          print(\"\\rIteration: {}/{} ({:.1f}%) epoch:{}  Loss: {:.5f}\".format(iteration, n_iterations_per_epoch, iteration * 100 / n_iterations_per_epoch, epoch+1, loss_train), end=\"\") \n",
        "          # print(\"\\r ({}) epoch:{}\".format( 'running', epoch+1), end=\"\")                        \n",
        "        preds_list, y_list = [], []\n",
        "        for iteration in range(1, n_iterations_test + 1):\n",
        "          X_batch, y_batch = mr_test1.next()             \n",
        "          probs = sess.run([activations],\n",
        "                    feed_dict={X:X_batch[:,:args.max_sent],\n",
        "                                is_training: False})\n",
        "          preds_list = preds_list + probs[0].tolist()\n",
        "          y_list = y_list + y_batch.tolist()\n",
        "\n",
        "        y_list = np.array(y_list)\n",
        "        preds_probs = np.array(preds_list)  \n",
        "        labels = np.argmax(y_list, axis=1)\n",
        "        predictions = np.argmax(preds_probs, axis=1)\n",
        "\n",
        "        accuracy_fold = accuracy_score(labels, predictions)\n",
        "        precision_fold = precision_score(labels, predictions, average='weighted', zero_division = 0 )\n",
        "        recall_fold = recall_score(labels, predictions, average='weighted')\n",
        "        f1_fold = f1_score(labels, predictions, average='weighted')\n",
        "        if best_f1 <= f1_fold :\n",
        "          best_accuracy = accuracy_fold\n",
        "          best_precision = precision_fold\n",
        "          best_recall = recall_fold\n",
        "          best_f1 = f1_fold\n",
        "        \n",
        "\n",
        "      acc_per_fold.append(best_accuracy)\n",
        "      precision_per_fold.append(best_precision)\n",
        "      recall_per_fold.append(best_recall)\n",
        "      f1_per_fold.append(best_f1)\n",
        "      print(\"\\rFold: {} accuracy: {:.4f}%  Precision: {:.4f} recall: {:.4f} F1: {:.4f}\".format(fold_no, best_accuracy, best_precision, best_recall, best_f1))\n",
        "      if args.loss_type == 'margin_loss':    \n",
        "            m = min(0.9, m + 0.1)\n",
        "      fold_no += 1\n",
        "\n",
        "accuracy = np.mean(acc_per_fold)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = np.mean(precision_per_fold)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = np.mean(recall_per_fold)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = np.mean(f1_per_fold)\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvB47yCsiNO1"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LZfpFV3xzCF"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4TosU5mxzCG"
      },
      "source": [
        "preds_list, y_list = [], []\n",
        "for iteration in range(1, n_iterations_test + 1):\n",
        "    X_batch, y_batch = mr_test.next()             \n",
        "    probs = sess.run([activations],\n",
        "            feed_dict={X:X_batch[:,:args.max_sent],\n",
        "                        is_training: False})\n",
        "    preds_list = preds_list + probs[0].tolist()\n",
        "    y_list = y_list + y_batch.tolist()\n",
        "        \n",
        "y_list = np.array(y_list)\n",
        "preds_probs = np.array(preds_list)                \n",
        "preds_probs[np.where( preds_probs >= threshold )] = 1.0\n",
        "preds_probs[np.where( preds_probs < threshold )] = 0.0 \n",
        "    \n",
        "# [precision, recall, F1, support] = precision_recall_fscore_support(y_list, preds_probs, average='samples')\n",
        "# acc = accuracy_score(y_list, preds_probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xkDR0CexzCH"
      },
      "source": [
        "print(len(preds_list))\n",
        "pred1 = preds_probs[:,1].tolist()\n",
        "pred11 = [int(i) for i in pred1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MQqOl-JxzCM"
      },
      "source": [
        "y_list1 = y_list.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPbnuPAGxzCN"
      },
      "source": [
        "# y_list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqvT8DJnxzCQ"
      },
      "source": [
        "accuracy = accuracy_score(y_list1, pred11)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_list1, pred11)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_list1, pred11)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_list1, pred11)\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmIcylyK6R0n"
      },
      "source": [
        "y_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3n2jIg50uuV"
      },
      "source": [
        "best_acc_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1n4heeEOvJR"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geUs-UQXOvZZ"
      },
      "source": [
        "!pip install transformers[torch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKTDpHDaOvkp"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOoNOqYBO4Z5"
      },
      "source": [
        "folder_path = '/content/drive/My Drive/Final Year Project/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/Bert/'\n",
        "folder_path_data =  '/content/drive/My Drive/Final Year Project/FYP/Sentiment Analysis/Implementation/'\n",
        "\n",
        "lankadeepa_data_path = folder_path_data + 'corpus/new/preprocess_from_isuru/lankadeepa_tagged_comments.csv'\n",
        "gossip_lanka_data_path = folder_path_data + 'corpus/new/preprocess_from_isuru/gossip_lanka_tagged_comments.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn1D5133O4XR"
      },
      "source": [
        "def embeds(model_output):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    return token_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52uspOE8O4UH"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(folder_path + 'xnli_output')\n",
        "model = BertModel.from_pretrained(folder_path + 'xnli_output', return_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGd-3z2PO-8a"
      },
      "source": [
        "import pandas as pd\n",
        "lankadeepa_data = pd.read_csv(lankadeepa_data_path)[:9059]\n",
        "gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n",
        "gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])\n",
        "\n",
        "all_data = pd.concat([lankadeepa_data,gossipLanka_data], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBXXG96AO_G0"
      },
      "source": [
        "sentences = all_data.comment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtJn35_sO4Rc"
      },
      "source": [
        "list_of_sentences = []\n",
        "for i in sentences:\n",
        "  list_of_sentences.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yng5apJMPHmp"
      },
      "source": [
        "encoded_input = tokenizer(list_of_sentences, padding=True, truncation=True, max_length=20, return_tensors='pt')\n",
        "\n",
        "#Compute token embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "#Perform pooling. In this case, mean pooling\n",
        "sentence_embeddings = embeds(model_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1z1sWgsPf_d"
      },
      "source": [
        "type(sentence_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvVbgIRtR852"
      },
      "source": [
        "Numpy_embedss = sentence_embeddings.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiBCd19iSivd"
      },
      "source": [
        "type(Numpy_embedss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIdvLTmHSkrh"
      },
      "source": [
        "!cd '/content/drive/My Drive/Final Year Project/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/Bert'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhmsxmLaSxRn"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save('context_embeds', Numpy_embedss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbN0Oj-PTQfu"
      },
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open(\"contextual_embeds\", 'wb') as file1:\n",
        "  pkl.dump(Numpy_embedss, file1)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H2w14iiWBtJ"
      },
      "source": [
        "!cp  -r  '/content/context_embeds.npy' '/content/drive/My Drive/Final Year Project/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/Bert'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTxBDB-fWh5r"
      },
      "source": [
        "!cp  -r  '/content/contextual_embeds' '/content/drive/My Drive/Final Year Project/FYP/Sentiment Analysis/Implementation/Sentiment Analysis/Bert'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5RzfMtjWrF4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}